{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_1.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_10.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_11.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_12.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_13.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_14.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_15.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_16.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_17.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_18.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_2.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_3.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_4.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_5.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_6.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_7.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_8.xlsx\n",
      "Processing file: E:\\厌氧消化预测大模型\\BP\\文献数据\\data_bp_9.xlsx\n",
      "Test set (10%) saved to: literatur_data(no_process)\\test_10_percent.jsonl, 964 entries\n",
      "Train set (30%) saved to: literatur_data(no_process)\\train_30_percent.jsonl, 2576 entries\n",
      "Train set (60%) saved to: literatur_data(no_process)\\train_60_percent.jsonl, 5161 entries\n",
      "Train set (90%) saved to: literatur_data(no_process)\\train_90_percent.jsonl, 7744 entries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "\n",
    "folder_path = \"E:\\厌氧消化预测大模型\\BP\\文献数据\"\n",
    "output_dir = \"literatur_data(no_process)\" \n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "test_jsonl = os.path.join(output_dir, \"test_10_percent.jsonl\") \n",
    "train_30_jsonl = os.path.join(output_dir, \"train_30_percent.jsonl\")  \n",
    "train_60_jsonl = os.path.join(output_dir, \"train_60_percent.jsonl\") \n",
    "train_90_jsonl = os.path.join(output_dir, \"train_90_percent.jsonl\")  \n",
    "excel_extensions = ('.xlsx', '.xls','csv')\n",
    "\n",
    "all_test_data = []\n",
    "all_train_30_data = []\n",
    "all_train_60_data = []\n",
    "all_train_90_data = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(excel_extensions):\n",
    " \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file: {file_path}\") \n",
    "        try:\n",
    "            excel_data = pd.read_excel(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "       \n",
    "\n",
    "        ### no numeric feature melt ###\n",
    "        numeric_cols = excel_data.select_dtypes(include=[np.number]).columns\n",
    "        feature_names = [col for col in numeric_cols if col != excel_data.columns[-1]]\n",
    "\n",
    "\n",
    "        output_col = excel_data.columns[-1]\n",
    "        json_list = []\n",
    "        for row_idx, row in excel_data.iterrows():\n",
    "            instruction = f\"Please predict {output_col} based on the following anaerobic digestion process description and process variable data.\"\n",
    "            for col_name in feature_names:\n",
    "                value = row[col_name]\n",
    "                if pd.notna(value) and str(value).lower() != \"nan\":\n",
    "                    instruction += f\" The {col_name} is {value}.\"\n",
    "\n",
    "         \n",
    "            output = str(row[output_col]) if pd.notna(row[output_col]) else \"\"\n",
    "\n",
    "      \n",
    "            json_obj = {\n",
    "                \"instruction\": instruction.strip(),\n",
    "                \"input\": \"\",\n",
    "                \"output\": output.strip()\n",
    "            }\n",
    "\n",
    "            json_list.append(json_obj)\n",
    "\n",
    "        if not json_list:\n",
    "            print(f\"No data processed for {file_path}. Skipping.\")\n",
    "            continue\n",
    "        train_data, test_data = train_test_split(json_list, test_size=0.1, random_state=42)\n",
    "        all_test_data.extend(test_data)\n",
    "        train_size = len(train_data)\n",
    "\n",
    "        n_30 = max(1, int(train_size * 0.3)) \n",
    "        n_60 = max(1, int(train_size * 0.6))\n",
    "        n_90 = max(1, int(train_size * 0.9))\n",
    "        train_30_data = random.sample(train_data, min(n_30, len(train_data)))\n",
    "        train_60_data = random.sample(train_data, min(n_60, len(train_data)))\n",
    "        train_90_data = random.sample(train_data, min(n_90, len(train_data)))\n",
    "        all_train_30_data.extend(train_30_data)\n",
    "        all_train_60_data.extend(train_60_data)\n",
    "        all_train_90_data.extend(train_90_data)\n",
    "with open(test_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for json_obj in all_test_data:\n",
    "        f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Test set (10%) saved to: {test_jsonl}, {len(all_test_data)} entries\")\n",
    "with open(train_30_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for json_obj in all_train_30_data:\n",
    "        f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Train set (30%) saved to: {train_30_jsonl}, {len(all_train_30_data)} entries\")\n",
    "with open(train_60_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for json_obj in all_train_60_data:\n",
    "        f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Train set (60%) saved to: {train_60_jsonl}, {len(all_train_60_data)} entries\")\n",
    "\n",
    "\n",
    "with open(train_90_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for json_obj in all_train_90_data:\n",
    "        f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Train set (90%) saved to: {train_90_jsonl}, {len(all_train_90_data)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
